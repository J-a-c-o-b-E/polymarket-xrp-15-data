# main.py (XRP up/down 15m, perfect 900 rows per session, uploads after session end)

import asyncio
import csv
import json
import os
import time
import threading
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Optional, Tuple, List

import httpx
import websockets

from google.oauth2.credentials import Credentials
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from googleapiclient.errors import HttpError


GAMMA_API = "https://gamma-api.polymarket.com"
CLOB_API = "https://clob.polymarket.com"
RTDS_WS_URL = "wss://ws-live-data.polymarket.com"
SCOPES = ["https://www.googleapis.com/auth/drive"]

# XRP
SLUG_URL = "https://polymarket.com/event/xrp-updown-15m-1766516400"
CHAINLINK_SYMBOL = "xrp/usd"

QUOTE_NOTIONALS_USDC = [1, 10, 100, 1000, 10000]

POLL_SECONDS = 1.0
MARKET_BUCKET_SECONDS = 15 * 60  # 900
REFRESH_SESSION_EVERY_SECONDS = 2.0
HTTP_TIMEOUT_SECONDS = 0.75

LOCAL_OUT_DIR = "session_csv"

WAIT_FOR_NEXT_FULL_SESSION = True  # start on the next full session boundary

LOG_EVERY_N = 1
LOG_PREVIEW_NOTIONALS = 3


@dataclass
class SessionState:
    slug: str
    up_token_id: str
    down_token_id: str


def utc_iso_now_seconds() -> str:
    return datetime.now(timezone.utc).isoformat(timespec="seconds")


def iso_utc_from_epoch_seconds(epoch_s: int) -> str:
    return datetime.fromtimestamp(epoch_s, tz=timezone.utc).isoformat(timespec="seconds")


def parse_slug(s: str) -> str:
    s = (s or "").strip()
    if "/event/" in s:
        return s.split("/event/", 1)[1].split("?", 1)[0].split("#", 1)[0].strip("/")
    if "/market/" in s:
        return s.split("/market/", 1)[1].split("?", 1)[0].split("#", 1)[0].strip("/")
    return s


def slug_prefix_from_slug(slug: str) -> str:
    parts = slug.rsplit("-", 1)
    return (parts[0] + "-") if len(parts) == 2 else (slug + "-")


def maybe_json(v):
    if v is None:
        return None
    if isinstance(v, (list, dict)):
        return v
    if isinstance(v, str):
        t = v.strip()
        if not t:
            return v
        if (t.startswith("[") and t.endswith("]")) or (t.startswith("{") and t.endswith("}")):
            try:
                return json.loads(t)
            except Exception:
                return v
    return v


def cents(x: Optional[float]) -> Optional[float]:
    return None if x is None else round(x * 100.0, 6)


class ChainlinkPriceFeed:
    def __init__(self, symbol: str):
        self.symbol = (symbol or "").lower()
        self._lock = threading.Lock()
        self.latest_price: Optional[float] = None
        self.latest_ts_ms: Optional[int] = None
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None

    def start(self):
        self._thread = threading.Thread(target=self._run_forever, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop.set()

    def get_latest(self) -> Tuple[Optional[float], Optional[int]]:
        with self._lock:
            return self.latest_price, self.latest_ts_ms

    def _run_forever(self):
        asyncio.run(self._ws_loop())

    async def _ws_loop(self):
        sub_msg = {
            "action": "subscribe",
            "subscriptions": [
                {"topic": "crypto_prices_chainlink", "type": "*", "filters": json.dumps({"symbol": self.symbol})}
            ],
        }
        backoff = 0.5
        while not self._stop.is_set():
            try:
                async with websockets.connect(RTDS_WS_URL, ping_interval=None) as ws:
                    await ws.send(json.dumps(sub_msg))
                    last_ping = time.time()

                    while not self._stop.is_set():
                        if time.time() - last_ping >= 5.0:
                            try:
                                await ws.ping()
                            except Exception:
                                break
                            last_ping = time.time()

                        try:
                            raw = await asyncio.wait_for(ws.recv(), timeout=2.0)
                        except asyncio.TimeoutError:
                            continue

                        try:
                            msg = json.loads(raw)
                        except Exception:
                            continue

                        if msg.get("topic") != "crypto_prices_chainlink":
                            continue

                        payload = msg.get("payload") or {}
                        if (payload.get("symbol") or "").lower() != self.symbol:
                            continue

                        value = payload.get("value")
                        ts_ms = payload.get("timestamp") or msg.get("timestamp")

                        if isinstance(value, (int, float)) and isinstance(ts_ms, int):
                            with self._lock:
                                self.latest_price = float(value)
                                self.latest_ts_ms = int(ts_ms)

                backoff = 0.5
            except Exception:
                await asyncio.sleep(backoff)
                backoff = min(backoff * 1.6, 10.0)


def make_http_client() -> httpx.Client:
    return httpx.Client(
        headers={"User-Agent": "pm-collector/1.0"},
        timeout=httpx.Timeout(HTTP_TIMEOUT_SECONDS),
        limits=httpx.Limits(max_connections=5, max_keepalive_connections=0),
        http2=False,
    )


def fetch_gamma_market_by_slug(http: httpx.Client, slug: str) -> Optional[dict]:
    try:
        r = http.get(
            f"{GAMMA_API}/markets",
            params={"slug": slug, "closed": False, "limit": 1, "offset": 0},
        )
        r.raise_for_status()
        data = r.json()
        if isinstance(data, list) and data:
            return data[0]
    except Exception:
        return None
    return None


def resolve_up_down_tokens(market: dict) -> Tuple[str, str]:
    outcomes = maybe_json(market.get("outcomes"))
    token_ids = maybe_json(market.get("clobTokenIds"))

    if not isinstance(outcomes, list) or not isinstance(token_ids, list) or len(outcomes) < 2 or len(token_ids) < 2:
        raise RuntimeError("gamma market missing outcomes or clobTokenIds")

    outcomes_norm = [str(x).strip().lower() for x in outcomes]
    token_norm = [str(x).strip() for x in token_ids]

    if "up" in outcomes_norm and "down" in outcomes_norm:
        up_id = token_norm[outcomes_norm.index("up")]
        down_id = token_norm[outcomes_norm.index("down")]
        return up_id, down_id

    return token_norm[0], token_norm[1]


def fetch_order_book_asks(http: httpx.Client, token_id: str) -> List[Tuple[float, float]]:
    r = http.get(f"{CLOB_API}/book", params={"token_id": token_id})
    r.raise_for_status()
    data = r.json()
    asks = data.get("asks") or []
    out: List[Tuple[float, float]] = []
    for a in asks:
        try:
            p = float(a["price"])
            s = float(a["size"])
            if p > 0 and s > 0:
                out.append((p, s))
        except Exception:
            pass
    out.sort(key=lambda x: x[0])
    return out


def avg_fill_prices_for_notionals(asks: List[Tuple[float, float]], notionals: List[float]) -> List[Optional[float]]:
    results: List[Optional[float]] = [None] * len(notionals)

    remaining = [float(n) for n in notionals]
    spent = [0.0 for _ in notionals]
    shares = [0.0 for _ in notionals]
    done = [False for _ in notionals]
    unfinished = len(notionals)

    for price, size in asks:
        if unfinished == 0:
            break

        level_cash = price * size

        for i in range(len(notionals)):
            if done[i]:
                continue

            if remaining[i] <= 1e-9:
                done[i] = True
                unfinished -= 1
                continue

            if level_cash <= remaining[i] + 1e-12:
                spent[i] += level_cash
                shares[i] += size
                remaining[i] -= level_cash
                if remaining[i] <= 1e-9:
                    done[i] = True
                    unfinished -= 1
            else:
                take_shares = remaining[i] / price
                spent[i] += remaining[i]
                shares[i] += take_shares
                remaining[i] = 0.0
                done[i] = True
                unfinished -= 1

    for i in range(len(notionals)):
        results[i] = (spent[i] / shares[i]) if shares[i] > 0 else None

    return results


class DriveUploader:
    def __init__(self):
        token_json = os.getenv("GOOGLE_OAUTH_TOKEN_JSON", "").strip()
        if not token_json:
            raise RuntimeError("missing GOOGLE_OAUTH_TOKEN_JSON")

        token_info = json.loads(token_json)
        creds = Credentials.from_authorized_user_info(token_info, scopes=SCOPES)

        if not creds.valid and creds.refresh_token:
            creds.refresh(Request())

        self.service = build("drive", "v3", credentials=creds, cache_discovery=False)

    def _list(self, q: str) -> List[dict]:
        res = self.service.files().list(
            q=q,
            fields="files(id,name,mimeType,parents)",
            pageSize=50,
            supportsAllDrives=True,
            includeItemsFromAllDrives=True,
        ).execute()
        return res.get("files", [])

    def upload_or_update(self, local_path: str, drive_folder_id: str, drive_filename: str):
        q = f"name='{drive_filename}' and '{drive_folder_id}' in parents and trashed=false"
        existing = self._list(q)

        media = MediaFileUpload(local_path, mimetype="text/csv", resumable=True)

        if existing:
            file_id = existing[0]["id"]
            self.service.files().update(
                fileId=file_id,
                media_body=media,
                supportsAllDrives=True,
            ).execute()
        else:
            metadata = {"name": drive_filename, "parents": [drive_folder_id]}
            self.service.files().create(
                body=metadata,
                media_body=media,
                fields="id",
                supportsAllDrives=True,
            ).execute()


def session_filename(prefix: str, bucket_ts: int) -> str:
    return f"{prefix}{bucket_ts}.csv"


def write_header(writer: csv.writer):
    headers = ["timestamp_utc"]
    for n in QUOTE_NOTIONALS_USDC:
        headers.append(f"up_buy_{n}_avg_cents")
        headers.append(f"down_buy_{n}_avg_cents")
    headers.append("xrp_chainlink_usd")
    writer.writerow(headers)


def sleep_until_epoch(target_epoch: float):
    while True:
        now = time.time()
        remaining = target_epoch - now
        if remaining <= 0:
            return
        if remaining > 0.2:
            time.sleep(min(remaining - 0.1, 0.5))
        else:
            time.sleep(min(remaining, 0.02))


def upload_with_logs(drive: DriveUploader, drive_folder_id: str, path: str, filename: str) -> bool:
    for attempt in range(5):
        try:
            drive.upload_or_update(path, drive_folder_id, filename)
            return True
        except HttpError as e:
            print(
                "upload_error_http",
                "attempt",
                attempt + 1,
                "status",
                getattr(e, "status_code", None),
                "error",
                str(e),
                flush=True,
            )
        except Exception as e:
            print("upload_error", "attempt", attempt + 1, "type", type(e).__name__, "error", repr(e), flush=True)
        time.sleep(0.5 * (2 ** attempt))
    return False


def main():
    drive_folder_id = os.getenv("DRIVE_FOLDER_ID", "").strip()
    if not drive_folder_id:
        raise RuntimeError("missing DRIVE_FOLDER_ID env var")

    os.makedirs(LOCAL_OUT_DIR, exist_ok=True)

    initial_slug = parse_slug(SLUG_URL)
    slug_prefix = slug_prefix_from_slug(initial_slug)

    drive = DriveUploader()

    test_path = os.path.join(LOCAL_OUT_DIR, "drive_test.csv")
    with open(test_path, "w", encoding="utf-8", newline="") as tf:
        tf.write("ok,utc\n")
        tf.write(f"1,{utc_iso_now_seconds()}\n")
    try:
        drive.upload_or_update(test_path, drive_folder_id, "drive_test.csv")
        print("drive_test_upload_ok", flush=True)
    except Exception as e:
        print("drive_test_upload_failed", type(e).__name__, repr(e), flush=True)

    feed = ChainlinkPriceFeed(CHAINLINK_SYMBOL)
    feed.start()

    http = make_http_client()

    print("drive_folder_id", drive_folder_id, flush=True)
    print("poll_seconds", POLL_SECONDS, flush=True)
    print("bucket_seconds", MARKET_BUCKET_SECONDS, flush=True)
    print("http_timeout_seconds", HTTP_TIMEOUT_SECONDS, flush=True)
    print("local_out_dir", os.path.abspath(LOCAL_OUT_DIR), flush=True)
    print("wait_for_next_full_session", WAIT_FOR_NEXT_FULL_SESSION, flush=True)

    now = time.time()
    cur_bucket = (int(now) // MARKET_BUCKET_SECONDS) * MARKET_BUCKET_SECONDS
    if WAIT_FOR_NEXT_FULL_SESSION and now > cur_bucket + 0.001:
        bucket_ts = cur_bucket + MARKET_BUCKET_SECONDS
    else:
        bucket_ts = cur_bucket

    try:
        while True:
            print("next_bucket", bucket_ts, "starts_at_utc", iso_utc_from_epoch_seconds(bucket_ts), flush=True)
            sleep_until_epoch(bucket_ts)

            slug = f"{slug_prefix}{bucket_ts}"
            state: Optional[SessionState] = None
            last_refresh = 0.0

            while state is None:
                now = time.time()
                if (now - last_refresh) >= REFRESH_SESSION_EVERY_SECONDS:
                    last_refresh = now
                    m = fetch_gamma_market_by_slug(http, slug)
                    if m:
                        try:
                            up_id, down_id = resolve_up_down_tokens(m)
                            state = SessionState(slug=slug, up_token_id=up_id, down_token_id=down_id)
                            print("switched_session", state.slug, flush=True)
                        except Exception:
                            pass
                if state is None:
                    time.sleep(0.1)

            local_file = os.path.join(LOCAL_OUT_DIR, session_filename(slug_prefix, bucket_ts))
            with open(local_file, "w", newline="", encoding="utf-8") as f:
                w = csv.writer(f)
                write_header(w)

                last_up_asks: List[Tuple[float, float]] = []
                last_down_asks: List[Tuple[float, float]] = []
                notionals = [float(x) for x in QUOTE_NOTIONALS_USDC]

                for i in range(MARKET_BUCKET_SECONDS):
                    target_epoch = bucket_ts + i
                    sleep_until_epoch(target_epoch)

                    try:
                        up_asks = fetch_order_book_asks(http, state.up_token_id)
                        if up_asks:
                            last_up_asks = up_asks
                    except Exception:
                        pass

                    try:
                        down_asks = fetch_order_book_asks(http, state.down_token_id)
                        if down_asks:
                            last_down_asks = down_asks
                    except Exception:
                        pass

                    up_avg = avg_fill_prices_for_notionals(last_up_asks, notionals) if last_up_asks else [None] * len(
                        notionals
                    )
                    down_avg = avg_fill_prices_for_notionals(
                        last_down_asks, notionals
                    ) if last_down_asks else [None] * len(notionals)

                    xrp_price, _ = feed.get_latest()
                    ts = iso_utc_from_epoch_seconds(int(target_epoch))

                    row = [ts]
                    for j in range(len(notionals)):
                        row.extend([cents(up_avg[j]), cents(down_avg[j])])
                    row.append(xrp_price)

                    w.writerow(row)

                    if (i % 10) == 0:
                        try:
                            f.flush()
                        except Exception:
                            pass

                    if LOG_EVERY_N > 0 and (i % LOG_EVERY_N) == 0:
                        show_n = max(0, min(LOG_PREVIEW_NOTIONALS, len(notionals)))
                        parts = [f"i={i+1}/{MARKET_BUCKET_SECONDS}", f"ts={ts}"]
                        for k in range(show_n):
                            parts.append(f"up{int(notionals[k])}={cents(up_avg[k])}c")
                            parts.append(f"dn{int(notionals[k])}={cents(down_avg[k])}c")
                        parts.append(f"xrp={xrp_price}")
                        print(" | ".join(parts), flush=True)

                try:
                    f.flush()
                except Exception:
                    pass

            filename = os.path.basename(local_file)
            ok = upload_with_logs(drive, drive_folder_id, local_file, filename)
            print("uploaded" if ok else "upload_failed", filename, flush=True)

            bucket_ts += MARKET_BUCKET_SECONDS

    except KeyboardInterrupt:
        pass
    finally:
        try:
            http.close()
        except Exception:
            pass
        feed.stop()


if __name__ == "__main__":
    main()
